\documentclass[12pt,a4paper]{article}

% --- Language / encoding (Overleaf: pdfLaTeX, UTF-8) ---
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}

% --- Math / theorem environments ---
\usepackage{amsmath,amssymb,amsthm,mathtools}

% --- Layout / misc ---
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{microtype}
\usepackage{enumitem}
\setlist{nosep}

% --- Links ---
\usepackage[hidelinks]{hyperref}

% --- Useful macros ---
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Cbb}{\mathbb{C}} % (НЕ \C: это акцент-команда в LaTeX)
\newcommand{\F}{\mathbb{F}}
\newcommand{\CN}{\mathcal{CN}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\inner}[2]{\left\langle #1,#2\right\rangle}
\newcommand{\given}{\mid}
\newcommand{\xtoP}{\xrightarrow{\Pbb}}
\newcommand{\logtwo}{\log_{2}}

% --- Theorem styles (Russian names, numbering by section) ---
\theoremstyle{definition}
\newtheorem{definition}{Определение}[section]
\newtheorem{remark}{Замечание}[section]

\theoremstyle{plain}
\newtheorem{theorem}{Теорема}[section]
\newtheorem{proposition}{Утверждение}[section]
\newtheorem{corollary}{Следствие}[section]

% --- Document ---
\title{Расширенный теорминимум по теории информации и кодированию}
\author{По Григорьеву А.А. Chatgpt 5.2 Thinking, Gemini 3.0 Pro\\ и немного Жданова Елисея} % чтобы убрать warning "No \author given"
\date{}

\begin{document}
\maketitle

\tableofcontents

\bigskip

\section*{Как читать документ}
Этот файл — сборник понятий из лекций. Определения тут расписаны плохо, а алгоритмы - без примеров работы. И вообще много лишних описаний. Но понятия тут покрывают теорминимум Григорьева + Бибикова. 

\section{Теория информации: энтропия Шеннона, Хартли и информационная дивергенция}

\subsection{Условная вероятность}
\begin{definition}[Условная вероятность]
Для событий $A,B$ при $\Pbb(A)>0$:
\[
\Pbb(B\given A)=\frac{\Pbb(A\cap B)}{\Pbb(A)}.
\]
\end{definition}
Интуиция из лекции: «после наступления $A$ вероятностное пространство как бы
сжимается до $A$», и мы нормируем меру на этом подмножестве.

\subsection{Совместное, маргинальные и условные распределения}
Пусть $X$ и $Y$ — дискретные случайные величины.

\begin{definition}[Совместное распределение]
\[
p_{X,Y}(x,y)=\Pbb(X=x,\;Y=y).
\]
\end{definition}

\begin{definition}[Маргинальные распределения]
\[
p_X(x)=\sum_{y}p_{X,Y}(x,y),\qquad
p_Y(y)=\sum_{x}p_{X,Y}(x,y).
\]
\end{definition}

\begin{definition}[Условные распределения]
При $p_X(x)>0$:
\[
p(y\given x)=\Pbb(Y=y\given X=x)=\frac{p_{X,Y}(x,y)}{p_X(x)}.
\]
Аналогично
\[
p(x\given y)=\frac{p_{X,Y}(x,y)}{p_Y(y)}
\quad\text{при }p_Y(y)>0.
\]
\end{definition}

В матричном представлении (часто рисовали на лекции): строки соответствуют $x$,
столбцы $y$, элементы — $p_{X,Y}(x,y)$.
Тогда маргиналы — суммы по строкам/столбцам.

А еще удобно интерпретировать как проекции из облака p(x, y) на координатные оси p(x) и p(y). (Бибиков А.М.)

\subsection{Формула Байеса, априорное/апостериорное, правдоподобие}
\begin{theorem}[Формула Байеса]
При $p_Y(y)>0$:
\[
p(x\given y)=\frac{p(x)\,p(y\given x)}{p(y)},
\qquad
p(y)=\sum_x p(x)p(y\given x).
\]
\end{theorem}

\begin{definition}[Априорное распределение]
$p(x)$ — распределение $X$ «до наблюдения» $Y$.
\end{definition}

\begin{definition}[Функция правдоподобия]
$p(y\given x)$, рассматриваемая как функция гипотезы $x$ при фиксированном наблюдении $y$.
\end{definition}

\begin{definition}[Апостериорное распределение]
$p(x\given y)$ — распределение $X$ после наблюдения $Y=y$.
\end{definition}

Лекционная интерпретация: «априорное» $\to$ «наблюдение через канал/датчик» $\to$
«апостериорное». Байес даёт точную формулу обновления.

\subsection{Энтропия Хартли}
\begin{definition}[Энтропия Хартли]
Если $\abs{\mathcal{X}}=M$ и $X$ равномерна на $\mathcal{X}$, то
\[
H_0(X)=\logtwo M.
\]
\end{definition}
Смысл: минимальное число бит, чтобы однозначно идентифицировать один из $M$
равновероятных исходов.

\subsection{Энтропия Шеннона}
\begin{definition}[Самоинформация и энтропия]
Самоинформация исхода $x$:
\[
I(x)=-\logtwo p(x).
\]
Энтропия Шеннона:
\[
H(X)=\E[I(X)]=-\sum_{x\in\mathcal{X}} p(x)\logtwo p(x).
\]
\end{definition}
Смысл (как в курсе). Энтропия — «средняя трудность угадывания» или «средняя
длина оптимального описания» при безошибочном сжатии (позже формализуется теоремой
Шеннона и кодами Хаффмана/арифметикой).

\subsection{Двоичная функция энтропии и $q$-ичная энтропия}
\begin{definition}[Двоичная энтропия]
\[
H_2(p)=-p\logtwo p-(1-p)\logtwo(1-p),\qquad p\in[0,1].
\]
\end{definition}

\subsection{Свойство группировки энтропии}
\begin{proposition}[Группировка]
Пусть исходы $X$ сгруппированы в классы $G$. Тогда
\[
H(X)=H(G)+\sum_{g}\Pbb(G=g)\,H(X\given G=g).
\]
\end{proposition}
Смысл: «сначала кодируем номер группы, потом внутри группы». Это фундаментальный
принцип композиции описаний.

\subsection{Аддитивность энтропии}
\begin{proposition}[Цепное правило для двух]
\[
H(X,Y)=H(X)+H(Y\given X)=H(Y)+H(X\given Y).
\]
\end{proposition}

\begin{corollary}[Аддитивность при независимости]
Если $X\perp Y$, то $H(Y\given X)=H(Y)$ и
\[
H(X,Y)=H(X)+H(Y).
\]
\end{corollary}

\subsection{Максимальность при равномерном распределении}
\begin{theorem}[Максимум энтропии на конечном алфавите]
Если $\abs{\mathcal{X}}=M$, то
\[
0\le H(X)\le \logtwo M,
\]
и $H(X)=\logtwo M$ тогда и только тогда, когда $X$ равномерна.
\end{theorem}

Лекционный путь доказательства через логарифмическое неравенство.
Обычно показывают, что для $p$ и равномерного $u$:
\[
D(p\|u)
=\sum_x p(x)\logtwo\frac{p(x)}{1/M}
=\sum_x p(x)\logtwo p(x)+\logtwo M
=\logtwo M-H(X)\ge 0,
\]
откуда $H(X)\le \logtwo M$.
Равенство $D=0$ означает $p=u$.

\subsection{Информационная дивергенция (дивергенция Кульбака–Лейблера)}
\begin{definition}[KL-дивергенция]
Для распределений $p,q$ на $\mathcal{X}$ (и условия $p(x)>0\Rightarrow q(x)>0$):
\[
D(p\|q)=\sum_{x\in\mathcal{X}} p(x)\logtwo\frac{p(x)}{q(x)}.
\]
\end{definition}

Смысл. $D(p\|q)$ измеряет «насколько дорого» кодировать данные, устроенные как $p$,
кодом, оптимальным для $q$.
В лекциях также подчёркивалось:
\begin{itemize}
\item $D(p\|q)\ge 0$ (неотрицательность).
\item $D(p\|q)=0 \Leftrightarrow p=q$ (на носителе $p$).
\item Не симметрична: $D(p\|q)\ne D(q\|p)$, не является метрикой.
\end{itemize}

\section{Информационная дивергенция, принцип максимума энтропии и границы Чебышёва}

\subsection{Выпуклая функция и неравенство Йенсена}
\begin{definition}[Выпуклость]
Функция $f$ выпукла на интервале, если
\[
f(\lambda x+(1-\lambda)y)\le \lambda f(x)+(1-\lambda)f(y),
\qquad \lambda\in[0,1].
\]
\end{definition}

\begin{theorem}[Йенсен]
Если $f$ выпукла и $\E|X|<\infty$, то
\[
f(\E X)\le \E f(X).
\]
\end{theorem}

Ключевой пример из курса. Функция $-\log x$ выпукла на $x>0$.
Это лежит в основе доказательства Гиббса (неотрицательности KL).

\subsection{Фундаментальное неравенство логарифма}
\begin{proposition}[Неравенство логарифма]
Для $x>0$:
\[
\ln x\le x-1,
\]
равенство только при $x=1$.
\end{proposition}
Часто используется как «локально лучшая линейная аппроксимация логарифма».

\subsection{Выпуклость множества распределений вероятностей}
\begin{proposition}
Множество всех распределений на конечном алфавите $\mathcal{X}$ выпукло:
если $p,q$ — распределения и $\lambda\in[0,1]$, то $r=\lambda p+(1-\lambda)q$ — распределение.
\end{proposition}

\subsection{Неотрицательность дивергенции (неравенство Гиббса)}
\begin{theorem}[Гиббс]
\[
D(p\|q)\ge 0.
\]
\end{theorem}

Идея доказательства (как на лекции через Йенсена).
Пишем
\[
D(p\|q)=\sum_x p(x)\logtwo\frac{p(x)}{q(x)}
=-\sum_x p(x)\logtwo\frac{q(x)}{p(x)}.
\]
Перейдём к натуральному логу (константа $\logtwo e$ роли не играет):
\[
-\sum_x p(x)\ln\frac{q(x)}{p(x)}=\E_p\!\left[-\ln\!\left(\frac{q(X)}{p(X)}\right)\right].
\]
Функция $-\ln(\cdot)$ выпукла, значит по Йенсену:
\[
\E_p[-\ln Z]\ge -\ln \E_p[Z],
\qquad Z=\frac{q(X)}{p(X)}.
\]
Но
\[
\E_p[Z]=\sum_x p(x)\frac{q(x)}{p(x)}=\sum_x q(x)=1,
\]
значит $\E_p[-\ln Z]\ge -\ln 1=0$, откуда $D(p\|q)\ge 0$.

\subsection{Условие равенства нулю}
\begin{proposition}
\[
D(p\|q)=0 \;\Longleftrightarrow\; p(x)=q(x)\ \text{для всех }x\text{, где }p(x)>0.
\]
\end{proposition}

\subsection{Метод множителей Лагранжа}
\begin{definition}[Лагранжиан]
Для задачи $\max f(x)$ при ограничениях $g_i(x)=c_i$:
\[
L(x,\lambda)=f(x)-\sum_i \lambda_i\,(g_i(x)-c_i).
\]
\end{definition}

\begin{definition}[Условие стационарности]
Необходимое условие экстремума:
\[
\nabla_x L(x,\lambda)=0,\qquad g_i(x)=c_i.
\]
\end{definition}

Геометрическая интерпретация.
В точке оптимума градиент цели лежит в линейной оболочке градиентов ограничений:
«уровень цели касается поверхности ограничений».

\subsection{Принцип максимума энтропии}
\begin{theorem}[Максимум энтропии при фиксированных ограничениях]
В реальном мире наиболее часто встречающимся распределением будет то, которое максимизирует энтропию
\[
p^\ast=\arg\max_{p\in\mathcal{P}} H(p).
\]
\end{theorem}

В лекциях это мотивировалось комбинаторно:
число реализаций макросостояния $\propto 2^{nH}$ (через формулу Стирлинга),
поэтому максимум энтропии соответствует «наиболее многочисленному» макросостоянию.

\subsection{Неравенство Чебышёва}
\begin{theorem}[Чебышёв]
Для случайной величины $X$ с $\E X=m$, $\Var(X)=\sigma^2$:
\[
\Pbb(\abs{X-m}\ge a)\le \frac{\sigma^2}{a^2}.
\]
\end{theorem}
Применение из курса: к среднему $\overline{X}_n$ независимых одинаково распределённых величин,
чтобы получить слабый ЗБЧ.

\section{Энтропия и пределы сжатия данных: от Чебышёва до теоремы Шеннона}

\subsection{Слабый закон больших чисел}
\begin{theorem}[Слабый ЗБЧ]
Если $X_1,\dots,X_n$ i.i.d., $\E X_i=m$, $\Var(X_i)=\sigma^2$, то
\[
\overline{X}_n=\frac{1}{n}\sum_{i=1}^{n} X_i \xtoP m.
\]
\end{theorem}
Доказательство в лекциях: Чебышёв к $\overline{X}_n$, так как $\Var(\overline{X}_n)=\sigma^2/n\to 0$.

\subsection{AEP (асимптотическая равнораспределённость)}
\begin{theorem}[AEP, формулировка]
Для i.i.d.\ источника $X^n$:
\[
-\frac{1}{n}\logtwo p(X^n) \xtoP H(X).
\]
\end{theorem}
Интуиция: почти вся вероятность сосредоточена на множестве $\approx 2^{nH}$
последовательностей, каждая из которых имеет вероятность порядка $2^{-nH}$.

\subsection{Типичное множество}
\begin{definition}[$\varepsilon$-типичное множество]
\[
T^{(n)}_{\varepsilon}
=\left\{x^n:\ \left|-\frac{1}{n}\logtwo p(x^n)-H(X)\right|<\varepsilon\right\}.
\]
\end{definition}

\begin{proposition}[Свойства типичности]
Для i.i.d.\ источника:
\begin{itemize}
\item $\Pbb(X^n\in T^{(n)}_{\varepsilon})\to 1$ при $n\to\infty$.
\item Для $x^n\in T^{(n)}_{\varepsilon}$:
\[
2^{-n(H+\varepsilon)}\le p(x^n)\le 2^{-n(H-\varepsilon)}.
\]
\item Следовательно, мощность типичного множества порядка $2^{nH}$:
\[
(1-o(1))\,2^{n(H-\varepsilon)}\ \lesssim\ \abs{T^{(n)}_{\varepsilon}}\ \lesssim\ 2^{n(H+\varepsilon)}.
\]
\end{itemize}
\end{proposition}

\subsection{Совместно типичные последовательности}
\begin{definition}[Совместная типичность]
Пара $(x^n,y^n)$ совместно типична, если одновременно типична по $p(x)$, $p(y)$ и $p(x,y)$
(эквивалентно: $-\frac{1}{n}\log p(x^n,y^n)\approx H(X,Y)$ и согласованы маргиналы/условные энтропии).
\end{definition}

Ключевой факт для доказательства прямой теоремы Шеннона:
если $X^n$ и $Y^n$ независимы,
то вероятность «случайно оказаться совместно типичными» экспоненциально мала
$\approx 2^{-n I(X;Y)}$.

% \subsection{Формула Стирлинга и метод перевала}
% \begin{proposition}[Стирлинг]
% \[
% n!\sim \sqrt{2\pi n}\left(\frac{n}{e}\right)^{n},
% \qquad
% \log n!=n\log n-n+O(\log n).
% \]
% \end{proposition}

% В курсе Стирлинг использовался для:
% \begin{itemize}
% \item оценок биномиальных коэффициентов $\binom{n}{k}$,
% \item вывода асимптотики $\binom{n}{pn}\approx 2^{nH_2(p)}$,
% \item связи вероятностей больших уклонений с KL-дивергенцией.
% \end{itemize}

% Метод перевала (смысл).
% Если нужно оценить сумму/интеграл вида $\sum e^{n\varphi(\cdot)}$ или $\int e^{n\varphi(t)}\,dt$,
% при больших $n$ главный вклад даёт окрестность максимума $\varphi$.
% Это техническая база для аккуратных асимптотик (в лекциях упоминалось как стандартный приём).

% \subsection{Граница Чернова и производящая функция моментов}
% \begin{definition}[MGF]
% Производящая функция моментов:
% \[
% M_X(t)=\E\left[e^{tX}\right]
% \quad (\text{если конечна в окрестности }0).
% \]
% \end{definition}

% \begin{theorem}[Граница Чернова]
% Для любого $t>0$:
% \[
% \Pbb(X\ge a)\le e^{-ta}\E[e^{tX}]=e^{-ta}M_X(t),
% \qquad
% \Rightarrow\quad
% \Pbb(X\ge a)\le \inf_{t>0} e^{-ta}M_X(t).
% \]
% \end{theorem}

% В лекциях это позиционировалось как «экспоненциально сильнее Чебышёва»
% и как шаг к типичным множествам и теоремам кодирования.

\section{Кодирование источника: энтропия, неравенство Крафта и алгоритм Хаффмана}

\subsection{Кодирование источника и энтропия источника}
\begin{definition}[Источник]
Дискретный источник задаётся распределением $p(x)$ на алфавите $\mathcal{X}$.
Энтропия источника $H(X)$ измеряет среднюю «информацию на символ».
\end{definition}

\begin{definition}[Кодирование источника]
Кодирование сопоставляет каждому символу $x\in\mathcal{X}$ кодовое слово над кодовым алфавитом
(обычно $\{0,1\}$).
Длина слова $l(x)$ — число бит.
\end{definition}

\subsection{Средняя длина и скорость кодирования}
\begin{definition}[Средняя длина]
\[
L=\sum_{x\in\mathcal{X}} p(x)\,l(x).
\]
\end{definition}

\begin{definition}[Скорость кодирования]
В безошибочном кодировании обычно сравнивают $L$ с $H(X)$.
Для блоков длины $n$ скорость часто пишут как «бит/символ»:
\[
R=\frac{\text{число бит на блок}}{n}.
\]
Для фиксированного кода по символам $R=L$.
\end{definition}

\subsection{Префиксный и однозначно декодируемый коды. Кодовое дерево}
\begin{definition}[Префиксный код]
Код префиксный, если ни одно кодовое слово не является префиксом другого.
\end{definition}

\begin{definition}[Однозначно декодируемый код]
Любая конкатенация кодовых слов декодируется единственным образом.
\end{definition}

Факт из курса: префиксный $\Rightarrow$ однозначно декодируемый;
удобство префиксных — «декодирование на лету» без разделителей.

Кодовое дерево.
Префиксный двоичный код соответствует двоичному дереву:
левое/правое ребро — $0/1$, лист — кодовое слово.
Префиксность означает: все слова соответствуют листьям.

\subsection{Неравенство Крафта}
\begin{theorem}[Крафт]
Для $2$-ичного префиксного кода длины $\{l(x)\}$ удовлетворяют
\[
\sum_{x\in\mathcal{X}} 2^{-l(x)}\le 1.
\]
\end{theorem}

\begin{theorem}[Крафт–Макмиллан (существование)]
Если задан набор целых длин $\{l(x)\}$, удовлетворяющий $\sum_x 2^{-l(x)}\le 1$,
то существует префиксный $2$-ичный код с этими длинами.
\end{theorem}

Смысл.
Крафт — «необходимое и достаточное» условие реализуемости длин как префиксного кода.

\subsection{Код Хаффмана и оптимальность}
\begin{definition}[Код Хаффмана]
Алгоритм Хаффмана строит оптимальное префиксное дерево,
итеративно объединяя два наименее вероятных символа (листа) в один узел с суммарной вероятностью,
пока не останется один корень.
\end{definition}

\begin{theorem}[Оптимальность Хаффмана]
Код Хаффмана минимизирует среднюю длину $L$ среди всех префиксных кодов
для данного распределения $p(x)$.
\end{theorem}

% Ключевые идеи доказательства (типичные для экзамена):
% \begin{itemize}
% \item В оптимальном дереве два наименее вероятных символа должны иметь максимальные длины
% и быть «соседними листьями» (общий родитель).
% \item После их объединения задача оптимизации редуцируется к задаче меньшего размера.
% \item Жадный выбор «двух минимальных» на каждом шаге оказывается корректным.
% \end{itemize}

\subsection{Граница Шеннона для средней длины и блоковое кодирование}
\begin{theorem}[Граница Шеннона на среднюю длину]
Для любого префиксного (однозначно декодируемого) двоичного кода:
\[
H(X)\le L.
\]
Существует код (например, Шеннона–Фано или Хаффмана), такой что
\[
L < H(X)+1.
\]
\end{theorem}

Блоковое кодирование.
Если кодировать блоки длины $n$ как символы супер-алфавита, то энтропия блока
$H(X^n)=nH(X)$, а средняя длина на символ может приближаться к $H(X)$
сколь угодно близко при больших $n$.

\section{Алгоритмы сжатия данных: от Хаффмана и Танстолла до арифметического и универсального кодирования Лемпеля–Зива}

% \subsection{Универсальное кодирование}
% \begin{definition}[Универсальное кодирование]
% Сжатие без знания истинного распределения источника (или при наличии зависимости/контекста),
% с целью асимптотически достигать энтропийной скорости:
% средняя длина на символ $\to H$ для широкого класса источников.
% \end{definition}

\subsection{Код Танстолла}
\begin{definition}[Код Танстолла]
Код Танстолла строит словарь фраз переменной длины (по входу) фиксированной мощности,
чтобы максимизировать среднюю длину фразы при фиксированном числе выходных бит.

\textbf{Лучши загптшить пример работы кода и посмотреть на него}
\end{definition}

Смысл, который часто спрашивают:
«у Хаффмана переменная длина кода, фиксированная длина входного символа;
у Танстолла наоборот — переменная длина входной фразы и фиксированная длина выходного индекса».

\subsection{Арифметическое кодирование}
\begin{definition}[Арифметическое кодирование]
Сообщение (последовательность символов) отображается в подотрезок $[0,1)$,
длина которого равна вероятности сообщения по выбранной модели.
Для передачи выбирается двоичный цилиндр (интервал двоичного разбиения),
целиком лежащий в этом подотрезке; его двоичная запись и есть код.

\textbf{Лучши загптшить пример работы кода и посмотреть на него}
\end{definition}

% Ключевой принцип.
% Если сообщение имеет вероятность $p$, то для указания интервала достаточно $\approx -\logtwo p$ бит.
% Поэтому при верной модели средняя длина близка к энтропии (в отличие от $+1$ у Хаффмана).

% Модели с контекстом.
% В лекциях подчёркивалось, что арифметическое кодирование естественно работает
% с общей вероятностной моделью вида
% \[
% p(x_1,x_2,\dots)=p(x_1)\,p(x_2\given x_1)\,p(x_3\given x_1,x_2)\cdots
% \]
% и «разбивает интервал по дереву контекстов».

\subsection{Алгоритмы Лемпеля–Зива, словарное и адаптивное сжатие}
\begin{definition}[Словарное сжатие]
Сжатие описывает поток как последовательность ссылок на ранее встречавшиеся подстроки/фразы
(явный или неявный словарь).
\end{definition}

\begin{definition}[LZ77 (идея)]
Скользящее окно: ищем самое длинное совпадение текущего префикса буфера
с подстрокой в окне и кодируем тройкой $(\text{offset}, \text{length}, \text{next})$.
\textbf{Лучши загптшить пример работы кода и посмотреть на него}
\end{definition}

\begin{definition}[Адаптивное кодирование]
Модель/словарь обновляются онлайн по мере чтения данных
(статистика не фиксирована заранее).
\end{definition}

% Теоретический смысл (из курса).
% LZ-семейство является универсальным:
% на достаточно длинных сообщениях средняя длина на символ стремится к энтропии источника
% для широких классов стационарных источников.

\section{Теория информации: условная энтропия и взаимная информация}

\subsection{Совместная и условная энтропия}
\begin{definition}[Совместная энтропия]
\[
H(X,Y)=-\sum_{x,y} p(x,y)\logtwo p(x,y).
\]
\end{definition}

\begin{definition}[Условная энтропия]
\[
H(X\given Y)=\sum_y p(y)H(X\given Y=y)
=-\sum_{x,y} p(x,y)\logtwo p(x\given y).
\]
\end{definition}

\subsection{Цепное правило для энтропии}
\begin{theorem}[Цепное правило]
\[
H(X,Y)=H(X)+H(Y\given X)=H(Y)+H(X\given Y).
\]
Более общо:
\[
H(X_1,\dots,X_n)=\sum_{i=1}^{n} H(X_i\given X_1,\dots,X_{i-1}).
\]
\end{theorem}

\subsection{Неравенство: условная энтропия не превосходит безусловную}
\begin{theorem}
\[
0\le H(X\given Y)\le H(X).
\]
\end{theorem}

Критерии равенств (важно для устного ответа):
\begin{itemize}
\item $H(X\given Y)=H(X)$ $\Longleftrightarrow$ $X\perp Y$
(наблюдение $Y$ не даёт информации о $X$).
\item $H(X\given Y)=0$ $\Longleftrightarrow$ $X=f(Y)$
(по $Y$ $X$ восстанавливается однозначно).
\end{itemize}

\subsection{Взаимная информация}
\begin{definition}[Взаимная информация]
\[
I(X;Y)=H(X)-H(X\given Y)=H(Y)-H(Y\given X)=H(X)+H(Y)-H(X,Y).
\]
\end{definition}

\begin{proposition}[Через KL-дивергенцию]
\[
I(X;Y)=\sum_{x,y} p(x,y)\logtwo\frac{p(x,y)}{p(x)p(y)}
=D\!\left(p_{X,Y}\,\big\|\,p_X p_Y\right)\ge 0.
\]
\end{proposition}

Интерпретация (как в лекции про каналы).
До наблюдения $Y$ неопределённость о $X$ равна $H(X)$.
После наблюдения $Y$ остаётся $H(X\given Y)$.
Разница $I(X;Y)$ — среднее «улучшение знания о $X$».

\section{Теория информации: взаимная информация, пропускная способность и абсолютная стойкость шифров}

\subsection{Канал, матрица переходных вероятностей и скорость передачи}
\begin{definition}[Дискретный канал]
Канал задаётся условными вероятностями $p(y\given x)$, $x\in\mathcal{X}$, $y\in\mathcal{Y}$.
При выборе входного распределения $p(x)$ получаем совместное:
\[
p(x,y)=p(x)p(y\given x),
\]
и выходное
\[
p(y)=\sum_x p(x)p(y\given x).
\]
\end{definition}

\begin{definition}[Скорость передачи информации]
В блочном кодировании: передаём $k$ информационных бит за $n$ использований канала:
\[
R=\frac{k}{n}\quad \text{(бит/использование канала)}.
\]
\end{definition}

\subsection{Пропускная способность канала}
\begin{definition}[Пропускная способность]
\[
C=\max_{p(x)} I(X;Y).
\]
\end{definition}
Лекционный смысл: $C$ — предельная достижимая скорость надёжной передачи
(при $R<C$ возможно $P_e\to 0$; при $R>C$ — невозможно).

\subsection{Канал без памяти}
\begin{definition}[Канал без памяти]
При $n$ последовательных использованиях:
\[
p(y^n\given x^n)=\prod_{i=1}^{n} p(y_i\given x_i).
\]
\end{definition}

\subsection{Цепь Маркова и лемма об обработке информации}
\begin{definition}[Марковская цепь]
$X\to Y\to Z$ — марковская цепь, если
\[
p(z\given x,y)=p(z\given y).
\]
\end{definition}

\begin{theorem}[Лемма об обработке информации (DPI)]
Если $X\to Y\to Z$, то
\[
I(X;Z)\le I(X;Y).
\]
\end{theorem}
Интуиция из лекции: «дополнительная обработка/пропуск через следующий канал не
может увеличить информацию о первоначальном входе».

\subsection{Абсолютная стойкость шифров (шифр Вернама)}
\begin{definition}[Шифр Вернама]
\[
C=M\oplus K,
\]
где ключ $K$ независим от сообщения $M$ и имеет ту же длину.
\end{definition}

\begin{proposition}[Абсолютная стойкость]
Если $K$ равномерный и независим от $M$, то
\[
I(M;C)=0.
\]
\end{proposition}

\begin{remark}
Классическое необходимое условие «энтропия ключа не меньше энтропии сообщения»:
\[
H(K)\ge H(M)
\]
для абсолютной стойкости (в разных формулировках).
\end{remark}

\section{Пропускная способность и фундаментальные пределы: неравенство Фано и обратная теорема}

\subsection{Неравенство Фано}
\begin{theorem}[Фано]. \textbf{Чето сложное, мб не обязательно учить}

Пусть $\hat{X}$ — оценка $X$, $P_e=\Pbb(\hat{X}\ne X)$, $\abs{\mathcal{X}}<\infty$.
Тогда
\[
H(X\given \hat{X})\le H_2(P_e)+P_e\logtwo(\abs{\mathcal{X}}-1).
\]
\end{theorem}
Как используется.
Если $P_e$ мало, то $H(X\given \hat{X})$ тоже мало: по оценке мы почти восстанавливаем $X$.

\subsection{Обратная теорема кодирования Шеннона (а это важно)}
\begin{theorem}[Обратная теорема (содержательная формулировка)]
Если $R>C$, то не существует последовательности кодов, обеспечивающей $P^{(n)}_e\to 0$ при $n\to\infty$.
Иначе говоря, надёжная передача со скоростью выше $C$ принципиально невозможна.
\end{theorem}

% Скелет доказательства (как в курсе).
% Берём сообщение $B$ (равномерно по $M=2^{nR}$ вариантам), декодированное $\hat{B}$,
% применяем Фано:
% \[
% H(B\given \hat{B})\le H_2(P_e)+P_e\logtwo(M-1).
% \]
% Далее
% \[
% I(B;\hat{B})=H(B)-H(B\given \hat{B})
% \ge nR-\bigl(H_2(P_e)+P_e\,nR\bigr).
% \]
% Но из свойств канала:
% \[
% I(B;\hat{B})\le I(X^n;Y^n)\le nC.
% \]
% При $R>C$ это заставляет $P_e$ быть ограниченной снизу (не может стремиться к нулю).

\section{Прямая теорема Шеннона: случайное кодирование, совместная типичность и примеры каналов}

\subsection{Прямая теорема кодирования Шеннона (achievability)}
\begin{theorem}[Прямая теорема]
Для канала без памяти при любой скорости $R<C$ существуют коды длины $n$,
такие что вероятность ошибки $P^{(n)}_e\to 0$ при $n\to\infty$.
\end{theorem}

% \subsection{Метод случайного кодирования}
% Идея из лекций.
% Вместо конструирования конкретного кода доказываем существование:
% выбираем кодовую книгу случайно по оптимальному $p(x)$,
% оцениваем среднюю вероятность ошибки по ансамблю,
% показываем, что она $\to 0$.
% Тогда существует конкретная реализация кода не хуже среднего.

% \subsection{Декодирование по совместной типичности}
% \begin{definition}[Декодирование по совместной типичности]
% Получив $y^n$, декодер ищет единственное сообщение $m$, для которого $(x^n(m),y^n)$ совместно типична.
% Если такого единственного нет — ошибка.
% \end{definition}

% Ключевой расчёт (как на лекции):
% \begin{itemize}
% \item переданное слово совместно типично с выходом с вероятностью $\to 1$;
% \item любое другое кодовое слово «случайно совместно типично» с вероятностью $\lesssim 2^{-nI(X;Y)}$;
% \item по объединению: $P_e \lesssim 2^{nR}\cdot 2^{-nI(X;Y)}$.
% При $R<I(X;Y)$ получаем $P_e\to 0$.
% Максимизируя $I$ по $p(x)$, получаем условие $R<C$.
% \end{itemize}

\subsection{Матрица переходных вероятностей}
\begin{definition}[Матрица канала]
Для конечных алфавитов канал задаётся таблицей $p(y\given x)$ (строки $x$, столбцы $y$).
Для входного $p(x)$ совместное $p(x,y)=p(x)p(y\given x)$, а выходное $p(y)=\sum_x p(x)p(y\given x)$.
\end{definition}

\subsection{Двоичный симметричный канал (BSC)}
\begin{definition}[BSC]
$\mathcal{X}=\mathcal{Y}=\{0,1\}$.
Ошибка бита с вероятностью $p$:
\[
p(y\given x)=
\begin{cases}
1-p,& y=x,\\
p,& y\ne x.
\end{cases}
\]
\end{definition}

\begin{proposition}[Пропускная способность BSC]
\[
C=1-H_2(p).
\]
\end{proposition}

Ключевые шаги (по лекции):
$H(Y\given X)=H_2(p)$ одинаково для всех $x$,
а равномерный вход даёт равномерный выход $H(Y)=1$.

\subsection{Двоичный канал со стиранием (BEC)}
\begin{definition}[BEC]
$\mathcal{X}=\{0,1\}$, $\mathcal{Y}=\{0,1,\bot\}$.
С вероятностью $p$ стирание $\bot$, иначе передаётся без ошибок.
\end{definition}

\begin{proposition}[Пропускная способность BEC]
\[
C=1-p.
\]
\end{proposition}

Интерпретация из лекции: стирание «честно сообщает незнание»,
поэтому при той же «частоте проблем» BEC может иметь большую ёмкость, чем BSC.

\subsection{$q$-ичный симметричный канал}
\begin{definition}[$q$-ичный симметричный канал]
Алфавит размера $q$.
Символ сохраняется с вероятностью $1-p$, иначе равновероятно превращается в один из $q-1$ других
(вероятность $p/(q-1)$).
\end{definition}

\begin{proposition}[Ёмкость $q$-ичного симметричного канала]
\[
C=\logtwo q - H(\text{распределение строки}).
\]
Эквивалентно часто пишут $C=\logtwo q - H_q(p)$ (с нужной конвенцией логарифма).
\end{proposition}

\section{Пропускная способность гауссовского канала: дифференциальная энтропия и формула Шеннона–Хартли}

\subsection{Дифференциальная энтропия}
\begin{definition}[Дифференциальная энтропия]
Для непрерывной случайной величины с плотностью $f$:
\[
h(X)=-\int_{\R} f(x)\logtwo f(x)\,dx.
\]
\end{definition}

\begin{remark}
Дифференциальная энтропия может быть отрицательной и зависит от масштаба (измерения).
Это нормально: смысл появляется в разностях вида
\[
I(X;Y)=h(Y)-h(Y\given X).
\]
\end{remark}

\subsection{Гауссовский канал и AWGN}
\begin{definition}[Вещественный AWGN-канал]
\[
Y=X+N,\qquad N\sim \N(0,\sigma^2),\qquad \E[X^2]\le P.
\]
\end{definition}

\begin{definition}[Комплексный AWGN-канал (как в лекциях про I/Q)]
\[
Y=C+W,\qquad W\sim \CN(0,N_0),\qquad \E|C|^2\le E_s.
\]
\end{definition}

\subsection{Максимальная энтропия при фиксированной дисперсии}
\begin{theorem}[Гауссовское распределение максимизирует $h$]
Среди всех распределений с фиксированной дисперсией $\Var(X)=\sigma^2$
дифференциальная энтропия $h(X)$ максимальна у гауссовского, и
\[
h(\N(0,\sigma^2))=\frac{1}{2}\logtwo(2\pi e \sigma^2).
\]
\end{theorem}
Это ключевой шаг для вывода ёмкости AWGN.

\subsection{Формула Шеннона–Хартли}
\begin{theorem}[Шеннон–Хартли]
Для AWGN-канала в полосе $B$ при мощности сигнала $P$ и спектральной плотности шума $N_0/2$:
\[
C = B\logtwo\!\left(1+\frac{P}{N_0 B}\right)\quad \text{бит/с}.
\]
\end{theorem}

В лекциях также появлялась «на символ» форма:
\[
C_{\text{sym}}=\logtwo(1+\mathrm{SNR}),
\qquad
\mathrm{SNR}=\frac{E_s}{N_0}.
\]

\subsection{Отношение сигнал/шум, спектральная эффективность и предел Найквиста}
\begin{definition}[SNR]
Типичные формы:
\[
\mathrm{SNR}=\frac{P}{N_0B}\ \text{(по мощности)},
\qquad
\mathrm{SNR}=\frac{E_s}{N_0}\ \text{(на символ)}.
\]
\end{definition}

\begin{definition}[Спектральная эффективность]
\[
\eta=\frac{R}{B}\quad \text{бит/с/Гц}.
\]
\end{definition}

\begin{definition}[Предел Найквиста (смысл)]
Число независимых степеней свободы сигнала в полосе $B$ за время $T$ порядка $2BT$ (вещественных).
Это даёт фундаментальное ограничение «символов в секунду на герц», которое обсуждалось
при переходе к $L_2$-модели сигналов.
\end{definition}

\subsection{Гильбертово пространство сигналов и ортогональность}
\begin{definition}[Пространство $L_2$]
Сигналы $x(t)$ рассматриваются как элементы $L_2$ с внутренним произведением
\[
\inner{x}{y}=\int x(t)y(t)\,dt.
\]
Норма $\norm{x}=\sqrt{\inner{x}{x}}$ соответствует корню из энергии.
\end{definition}

\begin{definition}[Ортогональность]
Сигналы $x,y$ ортогональны, если $\inner{x}{y}=0$.
\end{definition}

Ортонормированный базис $\{p_k(t)\}$ позволяет разлагать сигналы
\[
x(t)=\sum_k c_k p_k(t),
\]
а коэффициенты на приёме получаются согласованной фильтрацией:
\[
\hat{c}_k=\inner{y}{p_k}.
\]
Лекционный вывод: физический канал с AWGN сводится к набору независимых
гауссовских шумов в координатах базиса.

\section{Пропускная способность гауссовского канала и фундаментальный предел Шеннона}

\subsection{\texorpdfstring{Предел Шеннона и $E_b/N_0$}{Предел Шеннона и Eb/N0}}
\begin{definition}[Энергия на бит]
Если скорость $R$ (бит/с), мощность $P$, то
\[
E_b=\frac{P}{R}.
\]
\end{definition}

Связь с лекциями:
при малых скоростях/малой спектральной эффективности фундаментальный минимум
\[
\left(\frac{E_b}{N_0}\right)_{\min}=\ln 2 \approx 0.693 \ (\approx -1.59\ \text{dB}).
\]
Это «энергетическая граница Шеннона».

\subsection{Геометрическая интерпретация пропускной способности}
Идея «упаковки сфер».
Кодовые слова длины $n$ в AWGN можно рассматривать как точки в $\R^{n}$ (или $\Cbb^{n}$).
Шум добавляет случайный вектор; правильное декодирование означает,
что шум не «выбил» точку из её области решения.
При ML-декодировании области напоминают «шары» вокруг кодовых слов.
Ограничение на число кодовых слов связано с тем,
сколько маленьких шумовых шаров можно поместить в большой шар допустимых выходов.

\subsection{Эффект концентрации меры}
Содержание из лекций.
В больших размерностях:
\begin{itemize}
\item длина гауссовского шумового вектора концентрируется около $\sqrt{n\sigma^2}$;
\item объём шара концентрируется в тонком слое у поверхности;
\item «почти все» случайные векторы лежат близко к сфере фиксированного радиуса.
\end{itemize}
Эта геометрия и даёт «твёрдую» асимптотику границы $\log(1+\mathrm{SNR})$.

\section{Основы помехоустойчивого кодирования: гауссовские каналы, мягкие решения и метрики}

\subsection{Оптимальное решающее правило, ML и MAP}
\begin{definition}[Средняя вероятность ошибки]
Для решающего устройства $\hat{X}$ по наблюдению $Y$:
\[
P_e=\Pbb(\hat{X}\ne X).
\]
\end{definition}

\begin{theorem}[MAP-правило оптимально по $P_e$]
Решение
\[
\hat{x}(y)=\arg\max_x p(x\given y)
\]
минимизирует вероятность ошибки $P_e$.
\end{theorem}

\begin{corollary}[ML-правило]
Если априорное $p(x)$ равномерно, то MAP эквивалентно
\[
\hat{x}(y)=\arg\max_x p(y\given x)
\]
(максимум правдоподобия, ML).
\end{corollary}

\subsection{Мягкие и жёсткие решения}
\begin{definition}[Мягкие решения]
Декодер использует «насколько вероятно 0/1» (например, LLR),
а не только квантованный бит.
\end{definition}

\begin{definition}[Жёсткие решения]
Сначала наблюдение $Y$ сводится к дискретному $\tilde{Y}\in\{0,1\}$ (посимвольный детектор),
далее код декодируется в метрике Хэмминга.
\end{definition}

Лекционный акцент: мягкие решения обычно дают заметный выигрыш по требуемому $E_b/N_0$.

\subsection{Евклидово расстояние и метрика для AWGN}
Для AWGN:
\[
p(y\given c)\propto \exp\!\left(-\frac{\norm{y-c}^2}{2\sigma^2}\right),
\]
поэтому ML/MAP (при равномерном априоре по словам) сводится к
\[
\hat{c}=\arg\min_{c\in\mathcal{C}} \norm{y-c}^2.
\]
Это объясняет роль евклидовой метрики для «созвездий» и кодовых слов в гауссовском шуме.

\subsection{Расстояние Хэмминга, сферы и шары}
\begin{definition}[Расстояние Хэмминга]
$d_H(u,v)$ — число позиций, в которых $u$ и $v$ различны.
\end{definition}

\begin{definition}[Минимальное расстояние кода]
\[
d=\min_{c\ne c'} d_H(c,c').
\]
\end{definition}

\begin{definition}[Сфера/шар Хэмминга]
Число слов на расстоянии ровно $i$ от фиксированного:
\[
\binom{n}{i}(q-1)^i.
\]
Объём шара радиуса $t$:
\[
V_q(n,t)=\sum_{i=0}^{t} \binom{n}{i}(q-1)^i.
\]
\end{definition}

Ключевой факт:
код с расстоянием $d$ гарантированно исправляет $t=\lfloor(d-1)/2\rfloor$ ошибок
(при ближайшем соседе в Хэмминге).

\section{Блоковые коды: границы Хэмминга, Синглтона, Варшамова–Гилберта и введение в линейные коды}

\subsection{Параметры кода и скорость}
\begin{definition}[Блоковый код и параметры]
Код $\mathcal{C}\subset \mathcal{A}^n$ над алфавитом размера $q$.
Параметры $(n,M,d,q)$: длина $n$, мощность $M=\abs{\mathcal{C}}$,
минимальное расстояние $d$, размер алфавита $q$.
\end{definition}

\begin{definition}[Скорость и относительное расстояние]
\[
R=\frac{1}{n}\log_q M,\qquad \delta=\frac{d}{n}.
\]
\end{definition}

\subsection{Граница Хэмминга (упаковочная)}
\begin{theorem}[Граница Хэмминга]
Пусть код исправляет $t=\lfloor(d-1)/2\rfloor$ ошибок.
Тогда
\[
M\cdot V_q(n,t)\le q^n.
\]
\end{theorem}
Смысл: шары радиуса $t$ вокруг кодовых слов не пересекаются и лежат в $\mathcal{A}^n$ размера $q^n$.

\subsection{Граница Синглтона}
\begin{theorem}[Синглтон]
Если $M=q^k$, то
\[
d\le n-k+1.
\]
\end{theorem}

Интуиция: удалим $d-1$ координат;
разные кодовые слова не могут слиться, иначе расстояние было бы $\le d-1$.
Значит $M\le q^{n-(d-1)}$.

\subsection{Граница Варшамова–Гилберта (существование)}
\begin{theorem}[Варшамов–Гилберт, одна из форм]
Существует $q$-ичный код длины $n$ и расстояния $d$ мощности порядка
\[
M \gtrsim \frac{q^n}{V_q(n,d-1)}.
\]
\end{theorem}
Смысл: жадно выбираем кодовые слова, каждый раз выкидывая шар радиуса $d-1$;
пока место есть, можно выбирать новые слова.

\subsection{Введение в линейные коды}
Лекционный переход: для конструктивности и эффективного кодирования/декодирования
рассматривают линейные коды, где $\mathcal{C}$ — подпространство над $\F_q$.

\section{Линейные коды: порождающие и проверочные матрицы, коды Хэмминга и Рида–Соломона}

\subsection{Линейный код, вес и расстояние}
\begin{definition}[Линейный код]
$\mathcal{C}\subset \F_q^n$ — линейное подпространство размерности $k$.
Тогда $\abs{\mathcal{C}}=q^k$.
\end{definition}

\begin{definition}[Вес Хэмминга]
$w_H(c)=d_H(c,0)$.
Для линейного кода:
\[
d=\min_{c\ne 0} w_H(c).
\]
\end{definition}

\subsection{Порождающая и проверочная матрицы}
\begin{definition}[Порождающая матрица $G$]
Матрица $G\in \F_q^{k\times n}$ такая, что
\[
\mathcal{C}=\{uG:\ u\in\F_q^{k}\}.
\]
\end{definition}

\begin{definition}[Проверочная матрица $H$]
Матрица $H\in \F_q^{(n-k)\times n}$ такая, что
\[
\mathcal{C}=\{c\in \F_q^{n}:\ Hc^{\top}=0\}.
\]
\end{definition}

\begin{definition}[Синдром]
Для принятого $r=c+e$:
\[
s=Hr^{\top}=H(c+e)^{\top}=He^{\top},
\]
так как $Hc^{\top}=0$.
Синдром зависит только от ошибки.
\end{definition}

\begin{definition}[Систематический код]
Код систематический, если кодовое слово содержит исходные $k$ символов как подблок:
\[
G=[I_k\ P].
\]
\end{definition}

\subsection{Код Хэмминга}
\begin{definition}[Двоичный код Хэмминга]
Параметры:
\[
[2^m-1,\ 2^m-1-m,\ 3]_2.
\]
Исправляет $1$ ошибку.
\end{definition}

Строительство через $H$.
Проверочная матрица $H$ размера $m\times(2^m-1)$ состоит из всех ненулевых двоичных столбцов длины $m$.
Тогда:
\begin{itemize}
\item одиночная ошибка в позиции $i$ даёт синдром, равный $i$-му столбцу $H$, то есть её можно однозначно найти;
\item минимальное расстояние $d=3$.
\end{itemize}

\subsection{Коды Рида–Соломона, MDS и матрица Вандермонда}
\begin{definition}[RS-код (идея)]
Берём поле $\F_q$, выбираем $n$ различных точек $\alpha_1,\dots,\alpha_n$.
Информация — коэффициенты многочлена $f$ степени $<k$.
Кодовое слово:
\[
c=(f(\alpha_1),\dots,f(\alpha_n))\in\F_q^{n}.
\]
\end{definition}

\begin{proposition}[Параметры RS]
RS-код имеет параметры
\[
[n,k,n-k+1]_q,
\]
то есть является MDS и достигает границы Синглтона.
\end{proposition}

\begin{definition}[Матрица Вандермонда]
Матрица вида $V_{ij}=\alpha_j^{\,i-1}$.
Порождающая матрица RS-кода может быть записана как (укороченная) матрица Вандермонда
по точкам $\alpha_j$.
\end{definition}

\section{Свёрточные коды: алгоритмы Витерби, BCJR и принципы турбо-кодирования}

\subsection{Свёрточные коды и решётчатая диаграмма}
\begin{definition}[Свёрточный код]
Кодер имеет внутреннее состояние (память), обычно задаётся сдвиговым регистром длины $\nu$.
На каждом шаге принимает входной бит и выдаёт несколько выходных бит
(скорость, например, $1/2$).
\end{definition}

\begin{definition}[Решётчатая диаграмма (trellis)]
Граф по времени:
вершины — состояния на каждом шаге,
рёбра — возможные переходы при входных битах,
рёбра маркируются выходными символами и получают метрику/вероятность по наблюдениям канала.
\end{definition}

Ключевое отличие из лекции:
для свёрточных кодов trellis регулярна и число состояний ограничено $2^{\nu}$,
что делает ML/MAP-декодирование вычислимым.

\subsection{Алгоритм Витерби (ML-декодирование по пути)}
\begin{theorem}[Витерби, рекурсия динамического программирования]
Пусть метрика пути — сумма метрик рёбер.
Тогда минимальную метрику до состояния $s$ на шаге $t$ можно считать рекурсивно:
\[
M_{t+1}(s')=\min_{s\to s'} \bigl\{M_t(s)+m_t(s\to s')\bigr\}.
\]
Для каждого $s'$ сохраняется «выживший» предок,
затем по backtracking восстанавливается оптимальный путь.
\end{theorem}

Сложность.
Пропорциональна $n\cdot(\#\text{состояний})\cdot(\#\text{входящих рёбер})$.
Поэтому работает для ограниченного числа состояний,
но становится нереалистичной для произвольных блочных кодов с огромной решёткой
(отмечалось в лекции).

\subsection{Алгоритм BCJR (MAP/APP по битам, forward--backward)}
\begin{definition}[BCJR: факторы $\alpha$, $\beta$, $\gamma$]
Обычно вводят:
\begin{itemize}
\item $\alpha_t(s)$: вероятность прийти в состояние $s$ и наблюдать прошлые отсчёты;
\item $\beta_t(s)$: вероятность наблюдать будущие отсчёты при условии текущего состояния $s$;
\item $\gamma_t(s\to s')$: вероятность перехода $s\to s'$ (априорная) $\times$ правдоподобие наблюдения на шаге $t$.
\end{itemize}
\end{definition}

\begin{proposition}[BCJR-рекурсии]
\[
\alpha_{t+1}(s')=\sum_s \alpha_t(s)\,\gamma_t(s\to s'),
\qquad
\beta_t(s)=\sum_{s'} \gamma_t(s\to s')\,\beta_{t+1}(s').
\]
\end{proposition}

Оценка бита.
Апостериорная вероятность бита $b_t$ получается суммированием по тем рёбрам trellis на шаге $t$,
которые помечены $b_t=0$ или $b_t=1$,
с весами $\alpha_t(s)\gamma_t(s\to s')\beta_{t+1}(s')$.
В лекциях это объяснялось как «message passing» вдоль решётки.

\subsection{Турбо-коды и итеративное декодирование}
\begin{definition}[Турбо-код]
Состоит из двух (или более) относительно простых свёрточных кодеров,
соединённых через интерливинг (перестановку) входных бит.
\end{definition}

\begin{definition}[Итеративное декодирование]
Два MAP-декодера (BCJR) по очереди уточняют априорные вероятности битов,
обмениваясь «экстиринзик»-информацией; процесс повторяется несколько итераций.
\end{definition}

Интуиция (как в лекции).
Интерливинг «разрывает» локальные зависимости:
биты, соседние в одном кодере, становятся далёкими в другом.
Это помогает итеративному message passing лучше приближаться к глобальному MAP.

\subsection{Message passing (общий принцип)}
Смысл.
BCJR — частный случай алгоритма «sum-product» на фактор-графе:
локальные сообщения комбинируются и распространяются, что даёт эффективные маргиналы.
На деревьях это точно, на графах с циклами (как у турбо/LDPC) — приближённо,
но часто очень эффективно.

\section*{Финальные замечания(автора)}
\begin{itemize}
\item Удачи, А.М. Бибиков любит вопросы на понимание, но они простые, +- написать теорию и че-то понять прокатит

\end{document}
